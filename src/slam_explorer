#! /usr/bin/env python
import random

import rospy
from nav_msgs.msg import OccupancyGrid, Odometry
import numpy as np
import pandas as pd  # for saving the map to csv
import actionlib  # lib for placing the goal and robot autonomously navigating there
from move_base_msgs.msg import MoveBaseAction, MoveBaseGoal
from sensor_msgs.msg import LaserScan
from geometry_msgs.msg import Transform, TransformStamped, Vector3, Quaternion, Pose, Point
import tf2_ros  # lib for translating and rotating the frames
from random import randint
import copy
from math import sqrt, sin, cos, pi, atan2, asin
import skimage
import cv2
from visualization_msgs.msg import Marker  # for custom markers in RViz
from std_msgs.msg import Header, ColorRGBA  # for custom markers in RViz


# datatype for storing the cluster
class Cluster:

    def __init__(self, starting_point: dict, map_id: str, cluster_id: int, has_walls_inside_flag: bool = False):
        self.starting_point = starting_point  # starting point from which cluster is being searched further
        self.list_of_cells = np.asarray([[], []], dtype=int)  # list of cells of cluster
        self.map_id = map_id  # id of the map, on which clustering is conducted
        self.cluster_id = cluster_id  # enumerator of the cluster
        self.number_of_elements: int = 0  # number of cells in cluster
        self.cluster_centroid = {"j": None, "i": None}  # centroid of the cluster

        # Appending starting point to the cluster
        self.list_of_cells = np.concatenate((self.list_of_cells, np.asarray([[starting_point["j"]],
                                                                             [starting_point["i"]]])), axis=1)
        self.list_of_cells.astype(int, copy=False)

        # Flag if cluster has walls inside itself
        self.has_walls_inside_flag = has_walls_inside_flag

    def calculate_number_of_elements(self):
        self.number_of_elements = self.list_of_cells.shape[1]

    def calculate_centroid(self):
        # calculate the average coordinate:

        sum_coord_i = 0
        sum_coord_j = 0

        for cell in range(self.number_of_elements):
            sum_coord_j += self.list_of_cells[0][cell]
            sum_coord_i += self.list_of_cells[1][cell]

        self.cluster_centroid["i"] = int(sum_coord_i / self.number_of_elements)
        self.cluster_centroid["j"] = int(sum_coord_j / self.number_of_elements)

    def add_pixel(self, j, i):

        # Appending pixel to the cluster
        self.list_of_cells = np.concatenate((self.list_of_cells, np.asarray([[j], [i]])), axis=1)
        self.list_of_cells.astype(int, copy=False)


# Yamauchi's frontier detection method implemented in the class
class DFDdetectorClass:

    def __init__(self, min_size_frontier: float, percentage_cutoff: float):

        self.min_size_frontier = min_size_frontier  # minimum size of the frontier

        if percentage_cutoff > 1 or percentage_cutoff < 0:
            raise ValueError("DFD Error: The value of percentage cutoff should be between 0 and 1!")

        self.map_resolution: float = 0  # map resolution to calculate min_number_of_elements
        self.min_num_of_elements: int = 0  # will be calculated later

        self.percentage_cutoff = percentage_cutoff  # specifying how much top percentage
        # of gradient to left (0.3 = 70 %)

        self.idx = 1  # global index

        self.safe_boundary_between_goal_and_walls = 1   # [cells]

        self.raw_map_data_numpy_reshape = np.zeros((0, 0))   # global map for the class

    def map_gradient(self):
        map_I_copy = copy.deepcopy(self.raw_map_data_numpy_reshape)

        # initializing gradient magnitude of the map
        nmap_mag = np.zeros((map_I_copy.shape[0], map_I_copy.shape[1]), dtype="uint16")

        map_I_copy = map_I_copy.astype(np.int16)
        # assigning uint8 data type to the map, so value "-1" -> 255 and the biggest gradient is between -1:
        # not known and 100: occupied

        # changing values, so i'll get the biggest gradient between free cells: 0 and unknown: -1
        map_I_copy = np.where(map_I_copy == 100, 50, map_I_copy)  # Known Occupied: 100 -> 50
        map_I_copy = np.where(map_I_copy == -1, 100, map_I_copy)  # Unknown: -1 -> 100

        # Sobel kernel
        Gx = np.asarray([[-1, 0, 1], [-2, 0, 2], [-1, 0, 1]], dtype=np.int16)
        Gy = np.asarray([[-1, -2, -1], [0, 0, 0], [1, 2, 1]], dtype=np.int16)

        # Convoluting over whole map with mask, except 1 pixel border
        for j in range(1, map_I_copy.shape[0] - 2):
            for i in range(1, map_I_copy.shape[1] - 2):
                submap = map_I_copy[j - 1: j + 2, i - 1: i + 2]  # submap 3x3 pixels

                ncell_I_x = (submap * Gx).sum()  # element-wise multiplying and getting sum of all elements
                ncell_I_y = (submap * Gy).sum()  # element-wise multiplying and getting sum of all elements

                nmap_mag[j][i] = sqrt(
                    ncell_I_x ** 2 + ncell_I_y ** 2)  # get magnitude of gradient (we don't need theta)

        # Filtering out: leaving only the biggest gradient:
        max_gradient = np.amax(nmap_mag)  # get max value of gradient

        nmap_mag = np.where(nmap_mag < self.percentage_cutoff * max_gradient, 0, nmap_mag)  # filter out all cells,
        # except ones with
        # the highest gradient
        # (top 90%)

        nmap_mag = np.where(nmap_mag >= self.percentage_cutoff * max_gradient, 255, nmap_mag)  # make top highest
        # gradient:
        # 255, due to dtype
        # - highest
        # value (and for viz)

        nmap_mag = nmap_mag.astype(np.uint8)  # assing uint8 type to gradient map, because values are either 0 or 255

        return nmap_mag

    # Implements "Connected-Component Labeling" of binary image for clustering
    def clustering(self, nmap_mag, robot_position):

        cluster_list: list = []  # creating list, which will store all cluster objects
        nmap_mag_copy = copy.deepcopy(nmap_mag)  # making copy of magnitude map, to be able to delete pixels from it

        labeled_nmap_mag_copy = skimage.measure.label(nmap_mag_copy)    # CCL implemented labeling (returns ndarray:int)

        n_clusters = np.amax(labeled_nmap_mag_copy)     # number of clusters (0-th cluster is background)

        # DEBUG

        # cv2.namedWindow('CLUSTER ' + str(n_clusters), cv2.WINDOW_NORMAL)  # new window, named 'win_name'
        # cv2.imshow('CLUSTER ' + str(n_clusters), nmap_mag_copy)  # show image on window 'win_name' made of numpy.ndarray
        # cv2.resizeWindow('CLUSTER ' + str(n_clusters), 1600, 900)  # resizing window on my resolution
        #
        # cv2.waitKey(0)  # wait for key pressing
        # cv2.destroyAllWindows()  # close all windows

        # DEBUG END

        for i_cluster in range(1, n_clusters + 1):

            cluster_indices = np.where(labeled_nmap_mag_copy == i_cluster)

            starting_point = {"j": cluster_indices[0][0], "i": cluster_indices[1][0]}

            # checking if there is a wall in the cluster
            if np.any(self.raw_map_data_numpy_reshape[cluster_indices]) == 100:

                # doesn't add cluster to cluster list (deleting the cluster from nmap_mag_copy for visualisation)
                nmap_mag_copy[cluster_indices] = 0

            else:

                # initialising new cluster
                new_cluster = Cluster(starting_point, 'nmap_mag', i_cluster)

                # adding all cells to cluster
                for pixel in range(1, len(cluster_indices[0])):

                    new_cluster.add_pixel(cluster_indices[0][pixel], cluster_indices[1][pixel])

                new_cluster.calculate_number_of_elements()

                # if cluster is bigger than cluster_min_size -> calculate its centroid
                if new_cluster.number_of_elements > self.min_num_of_elements:

                    new_cluster.calculate_centroid()

                    cluster_list.append(copy.deepcopy(new_cluster))  # appending cluster to the cluster list

                # (deleting the cluster from nmap_mag_copy for visualisation)

                # DEBUG

                # if new_cluster.cluster_centroid["j"]:
                #     nmap_mag_copy[new_cluster.cluster_centroid["j"]][new_cluster.cluster_centroid["i"]] = 120
                #
                # cv2.namedWindow('CLUSTER '+str(i_cluster), cv2.WINDOW_NORMAL)  # new window, named 'win_name'
                # cv2.imshow('CLUSTER '+str(i_cluster), nmap_mag_copy)  # show image on window 'win_name' made of numpy.ndarray
                # cv2.resizeWindow('CLUSTER '+str(i_cluster), 1600, 900)  # resizing window on my resolution
                #
                # cv2.waitKey(0)  # wait for key pressing
                # cv2.destroyAllWindows()  # close all windows

                # DEBUG END

                # (deleting the cluster and its centroid from nmap_mag_copy for visualisation)
                nmap_mag_copy[cluster_indices] = 0

                if new_cluster.cluster_centroid["j"]:
                    nmap_mag_copy[new_cluster.cluster_centroid["j"]][new_cluster.cluster_centroid["i"]] = 0

        return cluster_list

    def frontier_detection_DFD(self, raw_map_data_numpy_reshape: np.ndarray,
                               robot_position_pix,
                               map_resolution: float, previous_map_reshape) -> dict:

        self.raw_map_data_numpy_reshape = raw_map_data_numpy_reshape

        self.map_resolution = map_resolution
        self.min_num_of_elements = int(self.min_size_frontier / self.map_resolution)

        gradient = self.map_gradient()
        cluster_list = self.clustering(gradient, robot_position_pix)

        distance_from_centroid = []

        # DEBUG

        # print("| CLUSTER ID | CENTROID  | DIST2CENT | ROBOT POS |  \n")
        # print("__________________________________________________  \n")

        # END OF DEBUG

        # computing distances to centroid for all clusters
        for cluster in cluster_list:

            c_j = cluster.cluster_centroid["j"]
            c_i = cluster.cluster_centroid["i"]
            centroid_neighbourhood = raw_map_data_numpy_reshape[c_j - self.safe_boundary_between_goal_and_walls:
                                                                c_j + self.safe_boundary_between_goal_and_walls,
                                                                c_i - self.safe_boundary_between_goal_and_walls:
                                                                c_i + self.safe_boundary_between_goal_and_walls]

            # Check if cluster centroid or any of its neighbours are in the wall(if in the wall -> remove,
            #                                                                   else -> calculate centroid):
            if np.all(centroid_neighbourhood != 100):
                # print("There is NO wall around!!!")
                distance_from_centroid.append(sqrt((robot_position_pix["i"] - cluster.cluster_centroid["i"]) ** 2 +
                                                   (robot_position_pix["j"] - cluster.cluster_centroid["j"]) ** 2))

                # DEBUG

                # print("|   ", cluster.cluster_id, "      |   [", [c_j, c_i], "]    |   ", distance_from_centroid[-1],
                #       "    |   ", [robot_position_pix["j"], robot_position_pix["i"]], "      |  \n")
                # print("______________________________________  \n")

                # END OF DEBUG

            else:
                # print("There is wall around!!!")
                cluster_list.remove(cluster)

        # sorting dict from min distance_from_centroid to max

        distance_from_centroid_sorted = copy.deepcopy(distance_from_centroid)
        distance_from_centroid_sorted.sort()

        # condition to go to other frontiers (not the closest one) if map doesn't change,
        # if map changes, go to the closest one

        if np.all(raw_map_data_numpy_reshape == previous_map_reshape):

            curr_distance = distance_from_centroid_sorted[self.idx]

            curr_distance_idx = distance_from_centroid.index(curr_distance)

            goal_coords_pix = {"i": cluster_list[curr_distance_idx].cluster_centroid["i"],
                               "j": cluster_list[curr_distance_idx].cluster_centroid["j"]}

            self.idx += 1

            # DEBUG

            # print("______________________________________  \n")
            # print("      THIS IS NOT CLOSEST FRONTIER      \n")
            # print("|   ", cluster_list[curr_distance_idx].cluster_id, "      |   [",
            #       [cluster_list[curr_distance_idx].cluster_centroid["j"],
            #        cluster_list[curr_distance_idx].cluster_centroid["i"]], "]    |   ",
            #       distance_from_centroid[curr_distance_idx], "    |   ",
            #       [robot_position_pix["j"], robot_position_pix["i"]], "      |  \n")
            # print("______________________________________  \n")

            # END OF DEBUG

        else:

            min_distance = distance_from_centroid_sorted[0]

            min_distance_idx = distance_from_centroid.index(min_distance)

            goal_coords_pix = {"i": cluster_list[min_distance_idx].cluster_centroid["i"],
                               "j": cluster_list[min_distance_idx].cluster_centroid["j"]}

            self.idx = 1  # zeroing the counter
            # DEBUG

            # print("______________________________________  \n")
            # print("      THIS IS CLOSEST FRONTIER          \n")
            # print("|   ", cluster_list[min_distance_idx].cluster_id, "      |   [",
            #       [cluster_list[min_distance_idx].cluster_centroid["j"],
            #        cluster_list[min_distance_idx].cluster_centroid["i"]], "]    |   ",
            #       distance_from_centroid[min_distance_idx], "    |   ",
            #       [robot_position_pix["j"], robot_position_pix["i"]], "      |  \n")
            # print("______________________________________  \n")

            # END OF DEBUG

        goal_coords_m = {"x": goal_coords_pix["i"]*map_resolution,
                         "y": goal_coords_pix["j"]*map_resolution}

        print("This is what comes out of DFD in [m]: \n", goal_coords_m)

        # DEBUG
        # raw_costmap_data_numpy_reshape_copy = copy.deepcopy(raw_costmap_data_numpy_reshape)
        # raw_costmap_data_numpy_reshape_copy = raw_costmap_data_numpy_reshape_copy.astype(np.uint8)
        # raw_costmap_data_numpy_reshape_copy[cluster_list[min_distance_idx].cluster_centroid["i"]][
        #     cluster_list[min_distance_idx].cluster_centroid["j"]] = 175
        #
        # name = "Goal: "+str(cluster_list[min_distance_idx].cluster_centroid["i"])+" "\
        #            + str(cluster_list[min_distance_idx].cluster_centroid["j"])
        #
        # cv2.namedWindow(name, cv2.WINDOW_NORMAL)  # new window, named 'win_name'
        # cv2.imshow(name, raw_costmap_data_numpy_reshape_copy)  # show image on window 'win_name' made of numpy.ndarray
        # cv2.resizeWindow(name, 1600, 900)  # resizing window on my resolution
        #
        # cv2.waitKey(0)  # wait for key pressing
        # cv2.destroyAllWindows()  # close all windows

        # END OF DEBUG

        return goal_coords_m

# class for storing the frontiers from FFD method
class FFDfrontier:

    def __init__(self):

        self.list_of_points = []        # intialise empty list of points ( point is {"i": xx, "j": xx})

        self.centroid = {"i": None, "j": None}      # initialise centroid attribute

        self.num_of_elements = 0                    # number of cells in frontier

    def add_point(self, point):

        self.list_of_points.append(point)

    def delete_all_points(self):

        self.list_of_points.clear()

    def calculate_num_of_elements(self):

        self.num_of_elements = self.list_of_points.__len__()

    def calculate_centroid(self):

        sum_coord_i = 0
        sum_coord_j = 0

        for point in self.list_of_points:

            sum_coord_i += point["i"]
            sum_coord_j += point["j"]

        sum_coord_i = int(sum_coord_i/self.list_of_points.__len__())     # get mean i coordinates of frontier
        sum_coord_j = int(sum_coord_j/self.list_of_points.__len__())     # get mean j coordinates of frontier

        self.centroid = {"i": sum_coord_i, "j": sum_coord_j}


# Fast Frontier Detection method implemented in the class
class FFDdetectorCLass:

    def __init__(self, debug_flag=False):

        self.laser_readings_polar = None                            # scan readings in polar coordinates
        self.laser_readings_car_orig_pix = None                     # scan readings in cartesian map frame (in pixels)
        self.map_msg_data_reshape = None                            # map
        self.map_res = None                                         # map resolution
        self.pos_of_robot = {"x": None, "y": None}                  # position of robot
        self.yaw_rot_of_robot = None                                # robot's rotation
        self.lines_list = []                                        # list of lines between scan points
        self.frontier_list = []                                     # list of frontiers
        self.frontier_idx = 1                                       # the index of frontier to go to (idx of goal front)
        self.debug_flag = debug_flag                                # shows/not shows debug outputs
        self.do_ffd_state = 0                                       # state enumerator for do_ffd()
        self.map_origin_pos = {"x": None, "y": None}                # position of [0, 0] cell in map frame
        self.global_pool_of_frontiers = []                          # stores found old frontiers

    def map_laser_readings(self, laser_readings_polar: np.ndarray):

        # Convert from polar coordinates to cartesian coordinates

        angle_of_reading = 0  # 360 laser readings in every message, each laser reading - int angle [0, 359]

        laser_readings_car_x = np.zeros(laser_readings_polar.shape)  # array of laser readings in x
        # cartesian coordinates

        laser_readings_car_y = np.zeros(laser_readings_polar.shape)  # array of laser readings in y
        # cartesian coordinates

        if self.debug_flag:
            # DEBUG

            img = copy.deepcopy(self.map_msg_data_reshape).astype(np.uint8)

            # change map from grayscale to bgr (map will stay the same, but i can add colours for debug)
            img_3d = cv2.cvtColor(img, cv2.COLOR_GRAY2RGB)

            # END DEBUG

        for laser_reading in laser_readings_polar:

            if laser_reading != float('inf'):

                laser_readings_car_x[angle_of_reading] = float(laser_reading) * cos(angle_of_reading / 180 * pi)

                laser_readings_car_y[angle_of_reading] = float(laser_reading) * sin(angle_of_reading / 180 * pi)

            else:

                laser_readings_car_x[angle_of_reading] = 0

                laser_readings_car_y[angle_of_reading] = 0

            angle_of_reading += 1  # increasing angle for next laser reading

        # deleting the 'inf' or [0, 0] scan values from array

        inf_idxs = np.where(np.logical_and(laser_readings_car_x == 0,
                                           laser_readings_car_y == 0))

        laser_readings_car_x = np.delete(laser_readings_car_x, inf_idxs)
        laser_readings_car_y = np.delete(laser_readings_car_y, inf_idxs)

        # stacking together laser x and y to common array

        laser_readings_car = np.vstack((laser_readings_car_x, laser_readings_car_y))

        # Transform /scan from robot's coordinate system to map (origin) coordinate system

        curr_laser_reading = {"x": None, "y": None}

        laser_readings_car_orig_x = np.zeros(laser_readings_car_x.shape)  # array of laser readings in x
        # origin cartesian coordinates

        laser_readings_car_orig_y = np.zeros(laser_readings_car_y.shape)  # array of laser readings in y
        # origin cartesian coordinates

        laser_readings_car_orig_i = np.zeros(laser_readings_car_x.shape)  # array of laser readings in i
        # origin cartesian coordinates

        laser_readings_car_orig_j = np.zeros(laser_readings_car_y.shape)  # array of laser readings in j
        # origin cartesian coordinates

        for laser_reading in range(0, laser_readings_car.shape[1]):

            # getting x,y coordinates in robot's coordinate system
            curr_laser_reading["x"] = laser_readings_car[:, laser_reading][0]
            curr_laser_reading["y"] = laser_readings_car[:, laser_reading][1]

            robot_frame_cords = np.array([[curr_laser_reading["x"]], [curr_laser_reading["y"]], [1]])

            # setting up the transformation matrix for transforming from robot's frame to the map (origin) frame
            transf_matx = np.array([[cos(self.yaw_rot_of_robot), -sin(self.yaw_rot_of_robot), self.pos_of_robot["x"]],
                                    [sin(self.yaw_rot_of_robot), cos(self.yaw_rot_of_robot), self.pos_of_robot["y"]],
                                    [0, 0, 1]])

            # getting new coords in map (origin) frame:

            origin_map_frame_coords = np.matmul(transf_matx, robot_frame_cords)

            laser_readings_car_orig_x[laser_reading] = origin_map_frame_coords[0][0]
            laser_readings_car_orig_y[laser_reading] = origin_map_frame_coords[1][0]

            laser_readings_car_orig_i[laser_reading] = int(origin_map_frame_coords[0][0]/self.map_res)
            laser_readings_car_orig_j[laser_reading] = int(origin_map_frame_coords[1][0]/self.map_res)

            if self.debug_flag:
                # DEBUG

                j = int(laser_readings_car_orig_y[laser_reading] / self.map_res)
                i = int(laser_readings_car_orig_x[laser_reading] / self.map_res)

                # getting the coordinates of the pixel, corresponding to the scan measurement
                coords_pix = {"j": j if j < self.map_msg_data_reshape.shape[0] - 1 else self.map_msg_data_reshape.shape[0] - 1,
                              "i": i if i < self.map_msg_data_reshape.shape[1] - 1 else self.map_msg_data_reshape.shape[1] - 1}

                cv2.namedWindow('map laser readings', cv2.WINDOW_NORMAL)  # new window, named 'win_name'

                img_3d[coords_pix["j"], coords_pix["i"], :] = np.array([255, 0, 0])

                cv2.imshow('map laser readings', img_3d)  # show image on window 'win_name' made of numpy.ndarray
                cv2.resizeWindow('map laser readings', 1600, 900)  # resizing window on my resolution

                cv2.waitKey(10)  # wait for key pressing
                # cv2.destroyAllWindows()  # close all windows

                # DEBUG END

        laser_readings_car_orig = np.vstack((laser_readings_car_orig_x, laser_readings_car_orig_y))
        laser_readings_car_orig_pix = np.vstack((laser_readings_car_orig_i, laser_readings_car_orig_j))
        laser_readings_car_orig_pix = laser_readings_car_orig_pix.astype(int)

        # leaving only unique values, saving the original indexes (without sorting)
        laser_readings_car_orig_pix = laser_readings_car_orig_pix[:, np.sort(np.unique(laser_readings_car_orig_pix,
                                                                                       return_index=True, axis=1)[1])]

        # returns laser readings in map (origin) frame
        return laser_readings_car_orig, laser_readings_car_orig_pix

    # function which gets line (all cells in line) from starting point (previous_point) and end point (curr_point)
    # (based on DDA algorithm)
    def DDALine(self, curr_point, previous_point):

        di = abs(curr_point["i"] - previous_point["i"])
        dj = abs(curr_point["j"] - previous_point["j"])

        steps = max(di, dj)

        if curr_point["i"] < previous_point["i"]:
            i_inc = -float(di/steps)
        else:
            i_inc = float(di/steps)

        if curr_point["j"] < previous_point["j"]:
            j_inc = -float(dj/steps)
        else:
            j_inc = float(dj/steps)

        points_list = [previous_point]  # creating list, containing first point in the line, to store the points
        new_point = {"i": previous_point["i"], "j": previous_point["j"]}

        if self.debug_flag:
            # DEBUG

            img = copy.deepcopy(self.map_msg_data_reshape).astype(np.uint8)

            # change map from grayscale to bgr (map will stay the same, but i can add colours for debug)
            img_3d = cv2.cvtColor(img, cv2.COLOR_GRAY2RGB)

            # END OF DEBUG

        for inc in range(0, steps):

            new_point["i"] += i_inc
            new_point["j"] += j_inc
            points_list.append({"i": int(new_point["i"]), "j": int(new_point["j"])})

            if self.debug_flag:
                # DEBUG

                i = int(new_point["i"])
                j = int(new_point["j"])
                # getting the coordinates of the pixel, corresponding to the scan measurement
                coords_pix = {"j": j if j < self.map_msg_data_reshape.shape[0] - 1 else self.map_msg_data_reshape.shape[0] - 1,
                              "i": i if i < self.map_msg_data_reshape.shape[1] - 1 else self.map_msg_data_reshape.shape[1] - 1}

                cv2.namedWindow('Drawing line', cv2.WINDOW_NORMAL)  # new window, named 'win_name'

                img_3d[coords_pix["j"], coords_pix["i"], :] = np.array([0, 0, 255])

                cv2.imshow('Drawing line', img_3d)  # show image on window 'win_name' made of numpy.ndarray
                cv2.resizeWindow('Drawing line', 1600, 900)  # resizing window on my resolution

                # if big line is drawn - hold a picture a little bit longer
                if inc == steps-1 and steps > 1:

                    cv2.waitKey(10)  # wait for key pressing

                else:

                    cv2.waitKey(100)  # wait for key pressing

                # cv2.destroyAllWindows()  # close all windows

                # DEBUG END

        return points_list

    def get_lines_from_laser_readings(self, laser_readings_car_orig_pix):

        # get the starting point from from first laser reading
        previous_point = {"i": laser_readings_car_orig_pix[:, 0][0],
                          "j": laser_readings_car_orig_pix[:, 0][1]}

        lines_list = []     # create empty list for lines

        # getting the line
        for laser_reading in range(1, laser_readings_car_orig_pix.shape[1]):

            curr_point = {"i": laser_readings_car_orig_pix[:, laser_reading][0],
                          "j": laser_readings_car_orig_pix[:, laser_reading][1]}

            lines_list.append(self.DDALine(curr_point, previous_point))

            previous_point = curr_point

        # getting last line from last point to first point
        curr_point = {"i": laser_readings_car_orig_pix[:, 0][0],
                      "j": laser_readings_car_orig_pix[:, 0][1]}

        lines_list.append(self.DDALine(curr_point, previous_point))

        return lines_list

    def check_neighbours(self, point):

        bounds = 1                      # boundaries for checking the neighbours (if 1 -> checks 3x3 submap)

        submap = self.map_msg_data_reshape[point["j"] - bounds: point["j"] + bounds + 1,
                 point["i"] - bounds: point["i"] + bounds + 1]

        unkn_flag = -1 in submap    # flag, indicating, there is unknown cells in the neighbouring cells in the map
        obst_flag = 100 in submap   # flag, indicating, there is an obstacle in the neighbouring cells in the map

        return unkn_flag, obst_flag

    def find_frontiers_in_lines(self, lines_list):

        frontier_list = []          # empty list to store frontiers
        new_frontier = FFDfrontier()

        if self.debug_flag:
            # DEBUG

            img = copy.deepcopy(self.map_msg_data_reshape).astype(np.uint8)

            # change map from grayscale to bgr (map will stay the same, but i can add colours for debug)
            img_3d = cv2.cvtColor(img, cv2.COLOR_GRAY2RGB)

            # END OF DEBUG

        # for cyclus to check all points in all lines to find frontiers
        for line in lines_list:

            for point in line:

                unkn_flag, obst_flag = self.check_neighbours(point)

                if unkn_flag and not obst_flag:

                    # add cell to frontier object
                    new_frontier.add_point(point)

                    if self.debug_flag:
                        # DEBUG

                        i = int(point["i"])
                        j = int(point["j"])
                        # getting the coordinates of the pixel, corresponding to the scan measurement
                        coords_pix = {"j": j if j < self.map_msg_data_reshape.shape[0] - 1 else self.map_msg_data_reshape.shape[0] - 1,
                                      "i": i if i < self.map_msg_data_reshape.shape[1] - 1 else self.map_msg_data_reshape.shape[1] - 1}

                        cv2.namedWindow('finding frontiers', cv2.WINDOW_NORMAL)  # new window, named 'win_name'

                        img_3d[coords_pix["j"], coords_pix["i"], :] = np.array([0, 0, 255])

                        cv2.imshow('finding frontiers', img_3d)  # show image on window 'win_name' made of numpy.ndarray
                        cv2.resizeWindow('finding frontiers', 1600, 900)  # resizing window on my resolution

                        cv2.waitKey(10)  # wait for key pressing
                        # cv2.destroyAllWindows()  # close all windows

                        # DEBUG END

                # if the frontier isn't empty
                if obst_flag and new_frontier.list_of_points:

                    # add finished frontier to the list of frontiers, and send signal
                    # that it's time to delete all points from frontier object
                    frontier_list.append(copy.deepcopy(new_frontier))
                    new_frontier.delete_all_points()

                if obst_flag:
                    if self.debug_flag:
                        # DEBUG

                        i = int(point["i"])
                        j = int(point["j"])
                        # getting the coordinates of the pixel, corresponding to the scan measurement
                        coords_pix = {
                            "j": j if j < self.map_msg_data_reshape.shape[0] - 1 else self.map_msg_data_reshape.shape[
                                                                                          0] - 1,
                            "i": i if i < self.map_msg_data_reshape.shape[1] - 1 else self.map_msg_data_reshape.shape[
                                                                                          1] - 1}

                        cv2.namedWindow('finding frontiers', cv2.WINDOW_NORMAL)  # new window, named 'win_name'

                        img_3d[coords_pix["j"], coords_pix["i"], :] = np.array([255, 0, 255])  # purple

                        cv2.imshow('finding frontiers', img_3d)  # show image on window 'win_name' made of numpy.ndarray
                        cv2.resizeWindow('finding frontiers', 1600, 900)  # resizing window on my resolution

                        cv2.waitKey(10)  # wait for key pressing
                        # cv2.destroyAllWindows()  # close all windows

                        # DEBUG END

                if not obst_flag and not unkn_flag:

                    if self.debug_flag:
                        # DEBUG

                        i = int(point["i"])
                        j = int(point["j"])
                        # getting the coordinates of the pixel, corresponding to the scan measurement
                        coords_pix = {
                            "j": j if j < self.map_msg_data_reshape.shape[0] - 1 else self.map_msg_data_reshape.shape[
                                                                                          0] - 1,
                            "i": i if i < self.map_msg_data_reshape.shape[1] - 1 else self.map_msg_data_reshape.shape[
                                                                                          1] - 1}

                        cv2.namedWindow('finding frontiers', cv2.WINDOW_NORMAL)  # new window, named 'win_name'

                        img_3d[coords_pix["j"], coords_pix["i"], :] = np.array([192, 192, 192])     # silver

                        cv2.imshow('finding frontiers', img_3d)  # show image on window 'win_name' made of numpy.ndarray
                        cv2.resizeWindow('finding frontiers', 1600, 900)  # resizing window on my resolution

                        cv2.waitKey(10)  # wait for key pressing
                        # cv2.destroyAllWindows()  # close all windows

                        # DEBUG END

        return frontier_list

    # function returning position of point in origin or map frame
    def transf_map2odom_and_back(self, from_which_frame: str, to_which_frame: str, point):

        if from_which_frame == "orig" and to_which_frame == "map":

            # x_or is position of point in origin frame
            if "x" and "y" in point.keys():

                x_or = {"x": point["x"],
                        "y": point["y"]}

            elif "i" and "j" in point.keys():

                x_or = {"x": point["i"] * self.map_res,
                        "y": point["j"] * self.map_res}

            x_map_m = {"x": x_or["x"] + self.map_origin_pos["x"],
                       "y": x_or["y"] + self.map_origin_pos["y"]}

            x_map_pix = {"i": int(x_map_m["x"]/self.map_res),
                         "j": int(x_map_m["y"]/self.map_res)}

            # returns position of point in map frame in meters
            return x_map_m, x_map_pix

        elif from_which_frame == "map" and to_which_frame == "odom":

            # x_map is position of point in map frame
            if "x" and "y" in point.keys():

                x_map = {"x": point["x"],
                         "y": point["y"]}

            elif "i" and "j" in point.keys():

                x_map = {"x": point["i"] * self.map_res,
                         "y": point["j"] * self.map_res}

            x_or_m = {"x": x_map["x"] - self.map_origin_pos["x"],
                      "y": x_map["y"] - self.map_origin_pos["y"]}

            x_or_pix = {"i": int(x_or_m["x"]/self.map_res),
                        "j": int(x_or_m["y"]/self.map_res)}

            # returns position of point on origin frame in meters
            return x_or_m, x_or_pix

    # main function of this class, using all of the above functions to find the goal and maintain found frontiers
    def do_ffd(self, pos_of_robot, yaw_rot_of_robot, laser_readings_polar, map_msg_data_reshape, previous_map, map_res,
               map_origin_position):

        # Getting actual data about robot's position and rotation
        self.pos_of_robot = pos_of_robot
        self.yaw_rot_of_robot = yaw_rot_of_robot

        # Getting actual map
        self.map_msg_data_reshape = map_msg_data_reshape
        self.map_res = map_res
        self.map_origin_pos = map_origin_position

        # Getting actual laser readings
        self.laser_readings_polar = laser_readings_polar

        # Maping laser readings to map (returning laser values mapped on map in pix)
        _, self.laser_readings_car_orig_pix = self.map_laser_readings(self.laser_readings_polar)

        # Getting lines between separate scan readings
        self.lines_list = self.get_lines_from_laser_readings(self.laser_readings_car_orig_pix)

        # Finding frontiers in lines
        self.frontier_list = self.find_frontiers_in_lines(self.lines_list)

        # Finding the centroids from all frontiers
        for frontier in self.frontier_list:
            frontier.calculate_centroid()  # calculating centroid for each frontier
            frontier.calculate_num_of_elements()

        # Sort frontiers from big to small
        self.frontier_list.sort(key=lambda x: x.num_of_elements, reverse=True)

        # if ffd algorithm found frontiers -> go to biggest one newly found
        if self.frontier_list:
            goal = {"x": self.frontier_list[0].centroid["i"] * self.map_res,
                    "y": self.frontier_list[0].centroid["j"] * self.map_res}

            del self.frontier_list[0]  # deleting frontier from frontier list

        elif not self.frontier_list and self.global_pool_of_frontiers or self.map_msg_data_reshape == previous_map:

            obst_flag = False
            unkn_flag = False

            while not obst_flag and unkn_flag:

                # transforming back frontier's centroid from map frame, [i, j] --> orig frame, [i, j]
                goal, self.global_pool_of_frontiers[0].centroid = \
                    self.transf_map2odom_and_back("map", "orig", self.global_pool_of_frontiers[0].centroid)
                # checking if frontier's centroid is useful and not explored goal
                unkn_flag, obst_flag = self.check_neighbours(self.global_pool_of_frontiers[0].centroid)
                del self.global_pool_of_frontiers[0]

        else:

            rospy.signal_shutdown("All Frontiers are visited")

        # TODO: here translating other frontiers centroids

        # Writing down found frontier to the global pool to use later if scan won't find any new frontiers

        for frontier in self.frontier_list:

            # transforming frontiers centroid from origin frame, [i, j] --> map frame, [i, j]
            _, frontier.centroid = self.transf_map2odom_and_back("orig", "map", frontier.centroid)
            # deleting all points, bcs centroid is already calculated and wrote down and it's all we need
            frontier.delete_all_points()
            # appending frontier with no points, only with found centroid in map frame
            self.global_pool_of_frontiers.append(frontier)

        #  from origin frame to /map frame and saving them global frontier pool

        self.frontier_idx += 1

        if self.debug_flag:
            # DEBUG

            print("         ||FFD||         ")
            print("+-----------+-----------+")
            print("| front idx | front num |")
            print("+-----------+-----------+")
            print("|   %d       |     %d     |" % (self.frontier_idx, self.frontier_list.__len__()))
            print("+-----------+-----------+")

            # DEBUG

            # DEBUG

            img = copy.deepcopy(self.map_msg_data_reshape).astype(np.uint8)

            # change map from grayscale to bgr (map will stay the same, but i can add colours for debug)
            img_3d = cv2.cvtColor(img, cv2.COLOR_GRAY2RGB)

            i = int(goal["x"] / self.map_res)
            j = int(goal["y"] / self.map_res)
            # getting the coordinates of the pixel, corresponding to the scan measurement
            coords_pix = {
                "j": j if j < self.map_msg_data_reshape.shape[0] - 1 else self.map_msg_data_reshape.shape[0] - 1,
                "i": i if i < self.map_msg_data_reshape.shape[1] - 1 else self.map_msg_data_reshape.shape[1] - 1}

            cv2.namedWindow('Goal', cv2.WINDOW_NORMAL)  # new window, named 'win_name'

            img_3d[coords_pix["j"], coords_pix["i"], :] = np.array([0, 255, 0])

            cv2.imshow('Goal', img_3d)  # show image on window 'win_name' made of numpy.ndarray
            cv2.resizeWindow('Goal', 1600, 900)  # resizing window on my resolution

            cv2.waitKey(1000)  # wait for key pressing
            cv2.destroyAllWindows()  # close all windows

            # DEBUG END

        return goal


class TurtleBotSlamExplorer:

    def __init__(self, frontier_detection_method):

        # DFD - Yamauchi Frontier Detection (YFD)
        # FFD - Fast Frontier Detection
        # WFD - Wavefront Frontier Detection
        self.frontier_detection_method = frontier_detection_method

        self.rate = rospy.Rate(5)  # rate of message sending is 10 Hz

        # Actionlib client definition #
        # move base is name of topic of the package, publishing the message on that topic allows you to move robot
        # in a desired position.
        # SimpleActionClient publishes messages on /move_base topic with format: MoveBaseAction
        self.action_client = actionlib.SimpleActionClient('/move_base', MoveBaseAction)
        self.action_client.wait_for_server()

        # tf2 package definition for translating and rotating frames
        self.tf2_broadcaster = tf2_ros.TransformBroadcaster()  # broadcasting new frames to the network
        self.tf2_buffer = tf2_ros.Buffer()  # stores all frames
        self.tf2_listener = tf2_ros.TransformListener(self.tf2_buffer)  # listens for new frames and calculating
        # translation and rotation between them

        self.status_buffer = np.zeros((10,), dtype=int)                 # stores last 10 statuses of the goal
        self.status_buffer_timestamp = [rospy.Duration(0.0), rospy.Duration(0.0), rospy.Duration(0.0), rospy.Duration(0.0),
                                        rospy.Duration(0.0), rospy.Duration(0.0), rospy.Duration(0.0), rospy.Duration(0.0),
                                        rospy.Duration(0.0), rospy.Duration(0.0)]   # stores timestamps of last 10 statuses

        self.first_goal: bool = True  # flag, which indicates, that robot goes
        # to it's first goal

        self.min_vel = 0.18  # [m/s] minimal speed of TurtleBot
        # set up in base_local_planner_params.yamls

        self.t_diff_goals = 0.1         # [s] due to goal slippering - required time difference between sending goals

        self.state = 0                  # state enumerator for state machine in self.explore()

    # FUNCTION, WHICH MAKES NEW FRAME BY TRANSFORMING FROM OLD AND SENDS IT OUT TO BUFFER
    def add_new_transformed_frame(self, parent_frame_id: str, child_frame_id: str, translation: dict, rotation: dict):

        new_transform_stamped = TransformStamped()  # initialise new transform object
        new_transform_stamped.header.stamp = rospy.Time.now()  # stamping it
        new_transform_stamped.header.frame_id = parent_frame_id  # adding source frame
        new_transform_stamped.child_frame_id = child_frame_id  # adding child frame

        # definition of new frame's rotation and translation parameters
        new_transform = Transform(
            translation=Vector3(
                x=translation["x"],
                y=translation["y"],
                z=translation["z"]),
            rotation=Quaternion(
                x=rotation["x"],
                y=rotation["y"],
                z=rotation["z"],
                w=rotation["w"]  # "1" - to save the orientation of new frame
            )

        )

        new_transform_stamped.transform = new_transform  # adding of transformation to object

        # sending out the transformed frame
        self.tf2_broadcaster.sendTransform(new_transform_stamped)

        return child_frame_id

    # FUNCTION SETS TWO NEW FRAMES ('CELL(0,0)' FRAME <- /map FRAME; 'GOAL' FRAME <- 'CELL(0,0)' FRAME) AND CALCULATES
    # TRANSFORMATION FROM ORIGIN /map FRAME TO 'GOAL' FRAME, THEN SEND TRANSFORMATION BETWEEN THEM
    # (WHICH IS EQUIVALENT TO MOVING THE MAP FRAME TO (0,0),
    # that's why it's called set_zeroed_frame_and_transform)
    def set_zeroed_map_frame_and_transform(self, raw_map: tuple, goal_coords: dict, parent_frame: str,
                                           child_frame: str):

        # --------------------------------------------------------------------------------------------------

        ################################
        # Transformation of the frames #
        ################################

        # (*) GETTING THE TRANSFORM FROM /map FRAME ORIGIN TO THE CELL(0,0) IN THE MAP (LOWER LEFT CORNER) #

        self.add_new_transformed_frame(parent_frame, "lower left corner",
                                       # x coordinates in [m] of cell(0,0) on the /map frame
                                       {"x": raw_map.info.origin.position.x,
                                        # y coordinates in [m] of cell(0,0) on the /map frame
                                        "y": raw_map.info.origin.position.y,
                                        "z": 0},
                                       {"x": 0, "y": 0, "z": 0, "w": 1})

        # (*) GETTING THE TRANSFORM FROM lower left corner FRAME ORIGIN TO THE goal coordinates #

        self.add_new_transformed_frame("lower left corner", child_frame,
                                       {"x": goal_coords["x"],
                                        "y": goal_coords["y"], "z": 0},
                                       {"x": 0, "y": 0, "z": 0, "w": 1})

        #######################################
        # End of Transformation of the frames #
        #######################################

        # --------------------------------------------------------------------------------------------------

        ######################################################################
        # Getting the translation between original /map frame and goal frame #
        ######################################################################

        # Getting 'map' and 'goal coordinates' frames from buffer and calculating transformation between them

        try:
            resulting_tranformation = self.tf2_buffer.lookup_transform('map',
                                                                       'goal coordinates',
                                                                       rospy.Time())
            transform_errors = False

            return resulting_tranformation, transform_errors

        except (tf2_ros.LookupException,
                tf2_ros.ConnectivityException,
                tf2_ros.ExtrapolationException):

            rospy.loginfo('ERROR: Lookup transform between frames!')
            transform_errors = True

            return None, transform_errors

        #############################################################################
        # End of Getting the translation between original /map frame and goal frame #
        #############################################################################

    # FOR NOW THIS IS NAIVE FRONTIER DETECTING ALGORITHM, RANDOMLY CHOSING UNKNOWN CELLS ON THE MAP
    def naive_frontier_detection(self, raw_map_data_numpy_reshape: np.ndarray,
                           raw_costmap_data_numpy_reshape: np.ndarray) -> dict:

        # goal is to send turtlebot to the cell of the map with coordinates [raw_map_numpy["x"], -//-["y"]]

        # searching of unknown cells in map with low cost of going there (doesn't have walls or angles around it)
        frontier_indices = np.where((raw_map_data_numpy_reshape == -1) &
                                    (raw_costmap_data_numpy_reshape < 20))

        if not frontier_indices:
            rospy.loginfo('ERROR: Frontier finding!')

        rnd_i = randint(0, np.shape(frontier_indices)[1] - 1)  # random choose from all rows and columns in
        # raw_map_..._reshape, which contains -1

        goal_coords = {"x": frontier_indices[1][rnd_i], "y": frontier_indices[0][rnd_i]}

        return goal_coords

    # get pose of robot in odom frame
    def get_pose_of_robot(self, raw_map_origin_pos, map_resolution):

        raw_odom_message = rospy.wait_for_message('/odom', Odometry)

        # vector of the robot position in odometry frame:
        x_od = {"x": raw_odom_message.pose.pose.position.x,
                "y": raw_odom_message.pose.pose.position.y}

        # DEBUG
        print("This is coordinates of robot in odom frame:\n", {"x": raw_odom_message.pose.pose.position.x,
                                                                "y": raw_odom_message.pose.pose.position.y})
        # END OF DEBUG

        # searching for transformation between "odom" frame and "map" frame
        try:
            tf_odom2map = self.tf2_buffer.lookup_transform("map",
                                                           "odom",
                                                           rospy.Time())
            # DEBUG
            print("This is transformation in map frame:\n", {"x": tf_odom2map.transform.translation.x,
                                                             "y": tf_odom2map.transform.translation.y})
            # END OF DEBUG

        except (tf2_ros.LookupException,
                tf2_ros.ConnectivityException,
                tf2_ros.ExtrapolationException):

            rospy.loginfo('ERROR: Lookup transform between frames while catching ROBOT position!')

        # if transformation was found -> OK, else assume that /map and /odom frames are the same
        if tf_odom2map:
            # vector of the translation of odom to map
            x_map2od = {"x": tf_odom2map.transform.translation.x,
                        "y": tf_odom2map.transform.translation.y}
        else:
            # vector of the translation of odom to map
            x_map2od = {"x": 0,
                        "y": 0}

        # vector of position of origin [0, 0] in the /map frame
        x_map2or = raw_map_origin_pos

        # finding vector of position of robot [m] in origin frame ( == pos of robot in or frame) by vector summation
        x_or_m = {"x": x_od["x"] + x_map2od["x"] - x_map2or["x"],
                  "y": x_od["y"] + x_map2od["y"] - x_map2or["y"]}

        # finding vector of position of robot [pix]
        x_or_pix = {"i": int(x_or_m["x"] / map_resolution),
                    "j": int(x_or_m["y"] / map_resolution)}

        # DEBUG
        print("This is coordinates of robot in !ORIGIN (lower_left_corner) frame [m]:\n", x_or_m)
        # END OF DEBUG

        return x_or_pix, x_or_m

    def get_yaw_rot_of_robot(self):

        """

        This is not my code!
        Source: https://automaticaddison.com/how-to-convert-a-quaternion-into-euler-angles-in-python/

        """

        """
        
        Convert a quaternion into euler angles (roll, pitch, yaw)
        roll is rotation around x in radians (counterclockwise)
        pitch is rotation around y in radians (counterclockwise)
        yaw is rotation around z in radians (counterclockwise)
        
        """

        raw_odom_message = rospy.wait_for_message('/odom', Odometry)

        x = raw_odom_message.pose.pose.orientation.x
        y = raw_odom_message.pose.pose.orientation.y
        z = raw_odom_message.pose.pose.orientation.z
        w = raw_odom_message.pose.pose.orientation.w

        t0 = +2.0 * (w * x + y * z)
        t1 = +1.0 - 2.0 * (x * x + y * y)
        roll_x = atan2(t0, t1)

        t2 = +2.0 * (w * y - z * x)
        t2 = +1.0 if t2 > +1.0 else t2
        t2 = -1.0 if t2 < -1.0 else t2
        pitch_y = asin(t2)

        t3 = +2.0 * (w * z + x * y)
        t4 = +1.0 - 2.0 * (y * y + z * z)
        yaw_z = atan2(t3, t4)

        return roll_x, pitch_y, yaw_z  # in radians

    # THIS IS MAIN FUNCTION OF CLASS, IT IMPLEMENTS AUTONOMOUS EXPLORATION
    def explore(self):

        while not rospy.is_shutdown():

            # "INIT" STATE
            if self.state == 0:

                print("_|___________________________________|_")
                print(" |     This is state: ", self.state, "            | ")
                print("_|___________________________________|_")

                if self.frontier_detection_method == "DFD":
                    # args: min frontier size is 0.2 [m^2], 0.9 - lower percentage cutoff is 90 [%]
                    frontier_detector = DFDdetectorClass(0.05, 0.9)       # init frontier detector class

                elif self.frontier_detection_method == "FFD":
                    frontier_detector = FFDdetectorCLass(True)                  # init frontier detector class

                raw_map = rospy.wait_for_message('/map', OccupancyGrid)  # get map with metadata
                raw_map_data_numpy = np.asarray(raw_map.data)  # 1d map -> np.ndarray
                previous_map_reshape = raw_map_data_numpy.reshape((raw_map.info.height,  # reshaping 1d np.ndarray
                                                                   raw_map.info.width))  # to 2d with right size
                # Occupancy grid explanation: -1 - unknown, 100 - Occupied, 0 - not occupied #
                self.state = 1             # goes to the next state

            # "SETTING UP GOAL IN ARRAY FRAME" STATE
            if self.state == 1:

                print("_|___________________________________|_")
                print(" |     This is state: ", self.state, "            | ")
                print("_|___________________________________|_")

                # FETCHING MAP #

                raw_map = rospy.wait_for_message('/map', OccupancyGrid)  # get map with metadata
                raw_map_data_numpy = np.asarray(raw_map.data)  # 1d map -> np.ndarray

                raw_map_data_numpy_reshape = raw_map_data_numpy.reshape((raw_map.info.height,  # reshaping 1d np.ndarray
                                                                         raw_map.info.width))  # to 2d with right size
                # Occupancy grid explanation: -1 - unknown, 100 - Occupied, 0 - not occupied #

                # get position of origin of the map in the /map frame
                raw_map_origin_pos = {"x": raw_map.info.origin.position.x,
                                      "y": raw_map.info.origin.position.y}

                # get raw map resolution [m/cell]
                raw_map_resolution = raw_map.info.resolution

                # FETCHING SCAN DATA #

                scan_msg = rospy.wait_for_message('/scan', LaserScan)

                # # DEBUG
                # print("(!!!) This is the type of Laser Scan data", type(scan_msg.ranges))
                # print("(!!!) This is Laser Scan data", type(scan_msg.ranges[0]))
                # # DEBUG END

                scan_data = np.array(scan_msg.ranges)

                # # DEBUG
                # print("(!!!!) This is the type of Laser Scan data NumPy", type(scan_data))
                # print("(!!!!) This is Laser Scan data NumPy", type(scan_data[0]))
                # print("(!!!!) Is inf python float?", scan_data[0] == float('Inf'))
                # print("(!!!!) This is shape of Laser Scan data", scan_data.shape)
                # # DEBUG END

                # get pose of robot in [pixels] and [m]
                pose_of_robot_pix, pose_of_robot_m = self.get_pose_of_robot(raw_map_origin_pos, raw_map_resolution)

                # # DEBUG
                #
                # self.add_new_transformed_frame("lower left corner", "fetched robot position",
                #                                # x coordinates in [m] of cell(0,0) on the /map frame
                #                                {"x": pose_of_robot_m["x"],
                #                                 # y coordinates in [m] of cell(0,0) on the /map frame
                #                                 "y": pose_of_robot_m["y"],
                #                                 "z": 0},
                #                                {"x": 0, "y": 0, "z": 0, "w": 1})
                #
                # # END OF DEBUG

                # get yaw rot of robot in [rad]:

                _, _, yaw_z = self.get_yaw_rot_of_robot()

                # # DEBUG
                # print("(***) Those are robot angles: \n-roll_x = %f \n-pitch_y = %f \n-yaw_z = %f"
                #       %(roll_x, pitch_y, yaw_z))
                # # END OF DEBUG

                # (!) FRONTIER DETECTION PART

                if self.frontier_detection_method == "DFD":

                    # # * DFD:

                    # get current coordinates of goal in [m]
                    goal_coords = frontier_detector.frontier_detection_DFD(raw_map_data_numpy_reshape,
                                                                           pose_of_robot_pix,
                                                                           raw_map.info.resolution,
                                                                           previous_map_reshape)

                if self.frontier_detection_method == "FFD":

                    # frontier_detector.do_ffd_state = 0                              # zeroing the state for do_ffd()
                    goal_coords = frontier_detector.do_ffd(pose_of_robot_m,
                                                           yaw_z,                   # yaw of robot
                                                           scan_data,               # raw scan data
                                                           raw_map_data_numpy_reshape,
                                                           previous_map_reshape,
                                                           raw_map.info.resolution,
                                                           raw_map_origin_pos)


                previous_map_reshape = raw_map_data_numpy_reshape

                self.state = 2      # going to the next state

            # "SETTING UP THE GOAL IN THE MAP FRAME" STATE
            if self.state == 2:

                print("_|___________________________________|_")
                print(" |     This is state: ", self.state, "            | ")
                print("_|___________________________________|_")

                # calculating the aproximation of travel time to the goal
                straight_distance = sqrt((goal_coords["x"] - pose_of_robot_m["x"]) ** 2 +
                                         (goal_coords["y"] - pose_of_robot_m["y"]) ** 2)

                approximate_travel_time = straight_distance / self.min_vel  # [s]
                if approximate_travel_time < 5:
                    approximate_travel_time = 5

                print('(*) This is goal coords:', goal_coords, "\n")

                # SETTING THE FRAMES AND RETURNING CALCULATED TRANSFORMATION

                resulting_transformation, transform_errors = self.set_zeroed_map_frame_and_transform(raw_map,
                                                                                                     goal_coords,
                                                                                                     "map",
                                                                                                     "goal coordinates")

                # checking if tf server made transformation
                if resulting_transformation:

                    self.state = 3          # going to the next state

                else:

                    pass                    # wait for the transformation
                    # self.state = 1          # going to the "SETTING UP GOAL IN ARRAY FRAME" state

            # "SENDING GOAL" STATE
            if self.state == 3:

                print("_|___________________________________|_")
                print(" |     This is state: ", self.state, "            | ")
                print("_|___________________________________|_")

                goal = MoveBaseGoal()
                goal.target_pose.header.frame_id = "map"
                goal.target_pose.header.stamp = rospy.Time.now()
                goal.target_pose.pose.position.x = resulting_transformation.transform.translation.x
                goal.target_pose.pose.position.y = resulting_transformation.transform.translation.y
                goal.target_pose.pose.orientation.w = 1.0

                # Send goal with timeout: args: (_, timeout for robot to move to the goal, timeout for robot to
                # accept the goal)
                self.action_client.send_goal_and_wait(goal, rospy.Duration(2 * approximate_travel_time),
                                                      rospy.Duration(0.0))

                self.state = 1  # go to the "SETTING UP GOAL IN ARRAY FRAME" state

            # REFRESHING THE GOAL STATUS BUFFER
            self.status_buffer_timestamp.append(rospy.Time.now())  # append last timestmp
            self.status_buffer = np.append(self.status_buffer,
                                           self.action_client.get_state())  # append last status
            # to the buffer
            self.status_buffer_timestamp.remove(self.status_buffer_timestamp[0])  # delete the oldest
            self.status_buffer = np.delete(self.status_buffer, 0)  # delete the oldest

            print("(*) This is last 10 goal statuses: ", self.status_buffer, "\n")
            print("(*) This is timestamps of last 10 goal statuses: ", self.status_buffer_timestamp, "\n")


def main():
    rospy.init_node('slam_explorer_n')

    # DFD - Yamauchi Frontier Detection (YFD)
    # FFD - Fast Frontier Detection
    # WFD - Wavefront Frontier Detection

    # slam_explorer = TurtleBotSlamExplorer("DFD")
    slam_explorer = TurtleBotSlamExplorer("FFD")
    slam_explorer.explore()


if __name__ == '__main__':
    try:
        main()
    except rospy.ROSInterruptException:
        pass
